# Projeto de Pipeline de Dados com HDFS/Hive - Processos Jur√≠dicos

![Linguagem](https://img.shields.io/badge/Linguagem-Python%20%7C%20HQL-blue)
![Tecnologia](https://img.shields.io/badge/Tecnologias-HDFS%20%7C%20Hive%20%7C%20Jupyter-yellow)
![Arquitetura](https://img.shields.io/badge/Arquitetura-Medallion%20(B%7CS%7CG)-brightgreen)
![Licen√ßa](https://img.shields.io/badge/Licen%C3%A7a-MIT-lightgrey)

## üìú Vis√£o Geral

Este projeto demonstra um pipeline completo de processamento de dados em batch, simulando o tratamento de um grande volume de dados processuais. O fluxo de trabalho engloba desde a ingest√£o de dados brutos at√© a cria√ß√£o de tabelas anal√≠ticas agregadas, prontas para consumo por ferramentas de Business Intelligence ou an√°lise de dados.

A arquitetura utilizada √© a **Medallion Architecture**, que organiza os dados em tr√™s camadas l√≥gicas: **Bronze** (bruto), **Silver** (limpo e transformado) e **Gold** (agregado e pronto para neg√≥cio).

---

## üèóÔ∏è Arquitetura do Pipeline

O fluxo de dados foi desenhado para garantir rastreabilidade, qualidade e performance, seguindo as etapas abaixo:

![Pipeline](pipeline.png)


1.  **Ingest√£o (Python/Jupyter)**: O notebook `Ingestao.ipynb` realiza o pr√©-processamento inicial, tratando inconsist√™ncias e gerando um arquivo CSV limpo.
2.  **Carregamento no HDFS**: O CSV √© transferido para o HDFS, servindo como fonte para a primeira camada do Data Lake.
3.  **Camada Bronze**: Os dados s√£o armazenados em seu formato bruto, como uma c√≥pia fiel da origem, garantindo um ponto de recupera√ß√£o.
4.  **Camada Silver**: Os dados s√£o limpos, transformados, enriquecidos e particionados para otimizar consultas futuras.
5.  **Camada Gold**: Os dados da camada Silver s√£o agregados para criar modelos de dados espec√≠ficos para as necessidades de neg√≥cio, como KPIs e m√©tricas de performance.

---

## üõ†Ô∏è Tecnologias Utilizadas

*   **Ingest√£o e Pr√©-processamento**: Python 3.12, Pandas, Jupyter Notebook
*   **Armazenamento Distribu√≠do**: HDFS (Hadoop Distributed File System)
*   **Data Warehousing e ETL**: Apache Hive (HiveQL)
*   **Formato de Armazenamento**: Parquet (para camadas Silver e Gold)

---

## üöÄ Como Executar

### Pr√©-requisitos
‚ö†Ô∏è Aten√ß√£o: Pr√©-requisito Obrigat√≥rio
*   A execu√ß√£o deste projeto requer o ambiente Docker do reposit√≥rio [**bigdata_docker**](https://github.com/fabiogjardim/bigdata_docker)
. Por favor, realize a instala√ß√£o e garanta que ele esteja em plena execu√ß√£o antes de continuar.

### Passo a Passo

1.  **Clone o reposit√≥rio**:
    ```bash
    git clone https://github.com/alexandre-s-costa/pipeline_processamento_massivo.git
    cd pipeline_processamento_massivo
    ```

2.  **Execute a Ingest√£o**:
    Abra e execute o notebook `Notebooks/Ingestao.ipynb` para gerar o arquivo `processos_bronze.csv` no diret√≥rio `/user/admin/datasets/cnj/bronze/`.

3.  **Carregue os dados no HDFS**:
    Crie o diret√≥rio no HDFS e copie o arquivo CSV para a camada Bronze.
    ```bash
    hdfs dfs -mkdir -p /user/admin/datasets/cnj/bronze
    hdfs dfs -put /tmp/datamart-2024.xlsx /user/admin/datasets/cnj/bronze
    ```
4.  **Execute os Scripts no Hue**:
    Execute os scripts HQL na ordem correta para criar e popular as tabelas.
    
---

## üß¨ Detalhes do Data Lake (Medallion Architecture)

### ü•â Camada Bronze: `bronze_processos`
- **Fun√ß√£o**: Armazenar os dados brutos ingeridos do CSV, servindo como *landing zone*.
- **Formato**: Tabela externa apontando para o arquivo CSV no HDFS.

<details>
<summary>Clique para ver o script HQL de cria√ß√£o</summary>

```sql
DROP TABLE IF EXISTS bronze_processos;

CREATE EXTERNAL TABLE bronze_processos (
  procedimento STRING,
  dt_baixa DATE,
  orgao_julgador STRING,
  sigla_grau STRING,
  dt_recebimento DATE,
  processoGeral STRING,
  classe STRING,
  id_classe STRING,
  dt_pendente_liquido STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/datalake/bronze/processos/'
TBLPROPERTIES ("skip.header.line.count"="1");
```

</details>

### ü•à Camada Silver: `silver_processos`
- **Fun√ß√£o**: Estruturar, limpar e enriquecer os dados. Nesta etapa, normalizamos datas, limpamos campos de texto e "explodimos" registros de pend√™ncias para an√°lise individual. A tabela √© particionada por ano e m√™s para otimizar a performance das consultas.
- **Formato**: Tabela gerenciada, armazenada em Parquet e particionada.

<details>
<summary>Clique para ver o script HQL de ETL</summary>

```sql
-- Cria√ß√£o da tabela (se n√£o existir)
CREATE TABLE IF NOT EXISTS silver_processos (
    procedimento            STRING,
    dt_baixa                DATE,
    orgao_julgador          STRING,
    sigla_grau              STRING,
    dt_recebimento          DATE,
    processoGeral           STRING,
    classe                  STRING,
    id_classe               STRING,
    dt_pendente_inicio      DATE,
    dt_pendente_fim         DATE
)
PARTITIONED BY (ano INT, mes INT)
STORED AS PARQUET;

-- Inser√ß√£o din√¢mica com transforma√ß√£o
SET hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE silver_processos PARTITION (ano, mes)
WITH cleaned AS (
    SELECT
        trim(regexp_replace(procedimento, '"', '')) AS procedimento,
        nullif(regexp_replace(dt_baixa, '"', ''), 'NULL') AS dt_baixa,
        trim(regexp_replace(orgao_julgador, '"', '')) AS orgao_julgador,
        trim(regexp_replace(sigla_grau, '"', '')) AS sigla_grau,
        nullif(regexp_replace(dt_recebimento, '"', ''), 'NULL') AS dt_recebimento,
        regexp_replace(regexp_replace(processoGeral, '"', ''), '[^0-9A-Za-z]', '') AS processoGeral,
        trim(regexp_replace(classe, '"', '')) AS classe,
        regexp_replace(regexp_replace(id_classe, '"', ''), '[{}]', '') AS id_classe,
        regexp_replace(regexp_replace(dt_pendente_liquido, '"', ''), '[{}]', '') AS dt_pendente_liquido
    FROM bronze_processos
),
exploded AS (
    SELECT
        c.*,
        pair
    FROM cleaned c
    LATERAL VIEW EXPLODE(split(c.dt_pendente_liquido, ',')) t AS pair
),
final AS (
    SELECT
        procedimento,
        CAST(dt_baixa AS DATE) AS dt_baixa,
        orgao_julgador,
        sigla_grau,
        CAST(dt_recebimento AS DATE) AS dt_recebimento,
        processoGeral,
        classe,
        id_classe,
        -- Converte a data de yyyyMMdd para o formato de data padr√£o
        to_date(from_unixtime(unix_timestamp(split(pair, ':')[0], 'yyyyMMdd'))) AS dt_pendente_inicio,
        to_date(from_unixtime(unix_timestamp(split(pair, ':')[1], 'yyyyMMdd'))) AS dt_pendente_fim
    FROM exploded
    WHERE split(pair, ':')[0] IS NOT NULL AND split(pair, ':')[1] IS NOT NULL -- Garante que o par de datas √© v√°lido
)
SELECT
    *,
    YEAR(dt_pendente_inicio) AS ano,
    MONTH(dt_pendente_inicio) AS mes
FROM final
WHERE dt_pendente_inicio IS NOT NULL;
```
</details>

### ü•á Camada Gold: Tabelas Anal√≠ticas
- **Fun√ß√£o**: Criar agrega√ß√µes e vis√µes de neg√≥cio para consumo final. S√£o tabelas menores, altamente otimizadas e que respondem a perguntas de neg√≥cio espec√≠ficas.
- **Formato**: Tabelas gerenciadas, armazenadas em Parquet.

#### `gold_tempo_por_orgao`
Calcula o tempo m√©dio de baixa e de pend√™ncia por √≥rg√£o julgador, m√™s e ano.

<details>
<summary>Clique para ver o script HQL</summary>

```sql
CREATE TABLE IF NOT EXISTS gold_tempo_por_orgao (
    orgao_julgador STRING,
    ano INT,
    mes INT,
    tempo_medio_baixa_dias INT,
    tempo_medio_pendencia_dias INT,
    total_processos INT
) STORED AS PARQUET;

INSERT OVERWRITE TABLE gold_tempo_por_orgao
SELECT
    orgao_julgador,
    ano,
    mes,
    AVG(datediff(dt_baixa, dt_recebimento)) AS tempo_medio_baixa_dias,
    AVG(datediff(dt_pendente_fim, dt_pendente_inicio)) AS tempo_medio_pendencia_dias,
    COUNT(*) AS total_processos
FROM silver_processos
WHERE ano IS NOT NULL AND mes IS NOT NULL
GROUP BY orgao_julgador, ano, mes;
```
</details>

#### `gold_classes_tempo_pendente`
Identifica as classes processuais com maior tempo m√©dio e m√°ximo de pend√™ncia.

<details>
<summary>Clique para ver o script HQL</summary>

```sql
CREATE TABLE IF NOT EXISTS gold_classes_tempo_pendente (
    classe STRING,
    ano INT,
    mes INT,
    tempo_medio_pendente_dias INT,
    tempo_maximo_pendente_dias INT,
    total_processos INT
) STORED AS PARQUET;

INSERT OVERWRITE TABLE gold_classes_tempo_pendente
SELECT
    classe,
    ano,
    mes,
    AVG(datediff(dt_pendente_fim, dt_pendente_inicio)) AS tempo_medio_pendente_dias,
    MAX(datediff(dt_pendente_fim, dt_pendente_inicio)) AS tempo_maximo_pendente_dias,
    COUNT(*) AS total_processos
FROM silver_processos
WHERE dt_pendente_inicio IS NOT NULL AND dt_pendente_fim IS NOT NULL
GROUP BY classe, ano, mes;
```
</details>

#### `gold_orgao_tempo_pendente`
Identifica os √≥rg√£os julgadores com maior tempo m√©dio e m√°ximo de pend√™ncia.

<details>
<summary>Clique para ver o script HQL</summary>

```sql
CREATE TABLE IF NOT EXISTS gold_orgao_tempo_pendente (
    orgao_julgador STRING,
    ano INT,
    mes INT,
    tempo_medio_pendente_dias INT,
    tempo_maximo_pendente_dias INT,
    total_processos INT
) STORED AS PARQUET;

INSERT OVERWRITE TABLE gold_orgao_tempo_pendente
SELECT
    orgao_julgador,
    ano,
    mes,
    AVG(datediff(dt_pendente_fim, dt_pendente_inicio)) AS tempo_medio_pendente_dias,
    MAX(datediff(dt_pendente_fim, dt_pendente_inicio)) AS tempo_maximo_pendente_dias,
    COUNT(*) AS total_processos
FROM silver_processos
WHERE dt_pendente_inicio IS NOT NULL AND dt_pendente_fim IS NOT NULL
GROUP BY orgao_julgador, ano, mes;